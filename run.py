# -*- coding: utf-8 -*-
import numpy as np
import pandas as pd
import torch
import os
import argparse
from sklearn.metrics import roc_auc_score
np.random.seed(2020)
torch.manual_seed(2020)

import scipy.sparse as sps
import optuna
import time
import pickle
from tqdm import tqdm

from dataset import load_data
from matrix_factorization_ori import ours_icdmw_DR, UMVUE_DR, MF
from itertools import product
from utils import gini_index, ndcg_func, get_user_wise_ctr, rating_mat_to_sample, binarize, shuffle, minU, recall_func, precision_func, f1_func

# ==================== å‘½ä»¤è¡Œå‚æ•°è§£æ ====================
def parse_args():
    parser = argparse.ArgumentParser(description='UMVUE-DR Hyperparameter Optimization')
    parser.add_argument('--dataset', type=str, default='kuai',
                       choices=['coat', 'yahoo', 'kuai'],
                       help='Dataset name')
    parser.add_argument('--trials', type=int, default=1000,
                       help='Number of optimization trials')
    parser.add_argument('--gpu', type=int, default=0,
                       help='GPU device ID')
    parser.add_argument('--batch_size', type=int, default=128,
                       help='Batch size for training')
    parser.add_argument('--batch_size_prop', type=int, default=2048,
                       help='Batch size for propensity model')
    parser.add_argument('--sampling_rate', type=float, default=1.0,
                       help='Training data sampling rate (0.0 to 1.0)')
    return parser.parse_args()

mse_func = lambda x,y: np.mean((x-y)**2)
acc_func = lambda x,y: np.sum(x == y) / len(x)

# ==================== å·¥å…·å‡½æ•° ====================

def save_results(filename, trial_number, params, results, training_time=None, epoch=None):
    """ä¿å­˜å•ä¸ªtrialçš„ç»“æœ"""
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(filename), exist_ok=True)
    
    with open(filename, 'a') as file:
        file.write(f'Trial {trial_number}\n')
        
        # ä¿å­˜å‚æ•°ï¼ˆç´§å‡‘æ ¼å¼ï¼‰
        param_str = ', '.join([f'{k}={v:.6f}' if isinstance(v, float) else f'{k}={v}' 
                               for k, v in params.items()])
        file.write(f'Params: {param_str}\n')
        
        # ä¿å­˜è®­ç»ƒä¿¡æ¯
        if training_time is not None:
            file.write(f'Training time: {training_time:.2f}s')
            if epoch is not None:
                file.write(f', Converged epoch: {epoch}')
            file.write('\n')
        
        # ä¿å­˜æŒ‡æ ‡
        for key, val in results.items():
            file.write(f'{key}: {val:.6f}\n')
        
        file.write('-' * 50 + '\n\n')


def save_best_summary(filename, study):
    """ä¿å­˜æœ€ä½³ç»“æœæ‘˜è¦"""
    with open(filename, 'a') as file:
        file.write('\n' + '=' * 80 + '\n')
        file.write('BEST TRIAL SUMMARY\n')
        file.write('=' * 80 + '\n')
        file.write(f'Trial number: {study.best_trial.number}\n')
        file.write(f'Best value: {study.best_trial.value:.6f}\n')
        file.write('\nBest parameters:\n')
        for key, value in study.best_trial.params.items():
            if isinstance(value, float):
                file.write(f'  {key}: {value:.6f}\n')
            else:
                file.write(f'  {key}: {value}\n')
        file.write('=' * 80 + '\n')


def generate_embeddings(x_train, y_train, x_test, y_test, num_user, num_item, 
                       embedding_k=32, dataset_name='coat', force_regenerate=False):
    """
    ç”Ÿæˆæˆ–åŠ è½½ç”¨æˆ·å’Œç‰©å“åµŒå…¥
    
    Args:
        force_regenerate: æ˜¯å¦å¼ºåˆ¶é‡æ–°ç”ŸæˆåµŒå…¥
    
    Returns:
        u_emb, i_emb, u_emb_test, i_emb_test
    """
    emb_file = f"saved/{dataset_name}_mfdrmc.pkl"
    
    # å°è¯•åŠ è½½å·²æœ‰åµŒå…¥
    if not force_regenerate:
        try:
            with open(emb_file, "rb") as f:
                u_emb = pickle.load(f)
                i_emb = pickle.load(f)
            print(f"  âœ… Loaded embeddings from {emb_file}")
            # å¤„ç†ä¸åŒç±»å‹çš„æ•°æ®
            if isinstance(u_emb, torch.Tensor):
                u_emb_test = u_emb.clone()
                i_emb_test = i_emb.clone()
            else:
                u_emb_test = u_emb.copy()
                i_emb_test = i_emb.copy()
            return u_emb, i_emb, u_emb_test, i_emb_test
        except FileNotFoundError:
            print(f"  âš ï¸ Embedding file not found, generating new embeddings...")
    
    # è®­ç»ƒMFæ¨¡å‹ç”ŸæˆåµŒå…¥
    print(f"  ğŸ”¨ Training MF model to generate embeddings (emb_dim={embedding_k})...")
    model = MF(num_user, num_item, batch_size=128, embedding_k=embedding_k)
    model.cuda()
    
    model.fit(
        x_train, y_train,
        lr=0.02,
        lamb=5e-4,
        tol=1e-5,
        verbose=False
    )
    
    # è¯„ä¼°MFæ¨¡å‹
    test_pred = model.predict(x_test)
    mse_mf = mse_func(y_test, test_pred)
    auc_mf = roc_auc_score(y_test, test_pred)
    ndcg_res = ndcg_func(model, x_test, y_test)
    recall_res = recall_func(model, x_test, y_test)
    
    print(f"  ğŸ“Š Base MF Performance:")
    print(f"     MSE: {mse_mf:.6f}, AUC: {auc_mf:.6f}")
    print(f"     NDCG@5: {np.mean(ndcg_res['ndcg_5']):.6f}, "
          f"NDCG@10: {np.mean(ndcg_res['ndcg_10']):.6f}")
    print(f"     Recall@5: {np.mean(recall_res['recall_5']):.6f}, "
          f"Recall@10: {np.mean(recall_res['recall_10']):.6f}")
    
    # æå–åµŒå…¥
    all_ui_user = np.array(list(product(np.arange(num_user), np.arange(1))))
    _, u_emb, _ = model.predict(all_ui_user, is_training=True)
    
    all_ui_item = np.array(list(product(np.arange(1), np.arange(num_item))))
    _, _, i_emb = model.predict(all_ui_item, is_training=True)
    
    # ä¿å­˜åµŒå…¥
    with open(emb_file, "wb") as f:
        pickle.dump(u_emb, f)
        pickle.dump(i_emb, f)
    print(f"  ğŸ’¾ Saved embeddings to {emb_file}")
    
    if isinstance(u_emb, torch.Tensor):
        u_emb_test = u_emb.clone()
        i_emb_test = i_emb.clone()
    else:
        u_emb_test = u_emb.copy()
        i_emb_test = i_emb.copy()
        
    return u_emb, i_emb, u_emb_test, i_emb_test


def generate_knn_matrix(dataset_name, x_train, y_train, x_test, y_test, 
                       num_user, num_item, u_emb, i_emb, k=10, force_regenerate=False):
    """
    ç”Ÿæˆæˆ–åŠ è½½KNNçŸ©é˜µ

    Args:
        force_regenerate: æ˜¯å¦å¼ºåˆ¶é‡æ–°ç”ŸæˆKNNçŸ©é˜µ
    """

    # å°è¯•åŠ è½½å·²æœ‰çš„KNNçŸ©é˜µ
    knn_file = f"saved/{dataset_name}_knn.pkl"
    if not force_regenerate:
        try:
            with open(knn_file, "rb") as f:
                knn_matrix = pickle.load(f)
            print(f"âœ… Loaded existing KNN matrix from {knn_file}")
            return knn_matrix
        except FileNotFoundError:
            print(f"âš ï¸ KNN matrix file not found, generating new KNN matrix...")

    # 1. å‡†å¤‡è§‚æµ‹çŸ©é˜µ
    obs = sps.csr_matrix(
        (np.ones(x_train.shape[0]), (x_train[:, 0], x_train[:, 1])),
        shape=(num_user, num_item),  # ä½¿ç”¨è®­ç»ƒé›†ç»´åº¦
        dtype=np.float32
    ).toarray()
    
    # 2. ç”Ÿæˆæ‰€æœ‰user-itemå¯¹
    def generate_total_sample(num_user, num_item):
        sample = []
        for i in range(num_user):
            sample.extend([[i,j] for j in range(num_item)])
        return np.array(sample)
    
    x_all = generate_total_sample(num_user, num_item)
    
    # 3. æ„å»ºåµŒå…¥çŸ©é˜µ
    embeddings = np.array([
        np.concatenate((u_emb[u], i_emb[i])) 
        for u, i in x_all
    ]).reshape(num_user, num_item, 64)
    
    # 4. é‡‡æ ·æ•°æ®
    ul_idxs = np.arange(len(x_all))
    np.random.shuffle(ul_idxs)
    x_all_idx = ul_idxs[:3*len(x_train)]
    x_sampled = x_all[x_all_idx]
    x_sampled = np.r_[x_sampled, x_train]
    
    # 5. è®¡ç®—KNNçŸ©é˜µ
    def find_k_nearest_neighbors(embeddings, k, x_train, x_sampled):
        neighbors = []
        # ç¡®ä¿ç´¢å¼•æ˜¯æ•´æ•°ç±»å‹
        x_sampled_int = x_sampled.astype(int)
        x_train_int = x_train.astype(int)

        embeddings_temp = embeddings[x_sampled_int[:, 0], x_sampled_int[:, 1]]
        original_dict = {}
        for i in range(len(embeddings_temp)):
            original_dict[i] = [x_sampled_int[i][0], x_sampled_int[i][1]]
        
        # æ·»åŠ è¿›åº¦æ¡
        for i in tqdm(x_train_int, desc="Computing KNN neighbors", leave=False):
            distances = np.linalg.norm(embeddings_temp - embeddings[i[0], i[1]], axis=1)
            nearest_indices = np.argsort(distances)[1:k+1]
            nearest_values = [obs[original_dict[idx][0], original_dict[idx][1]] for idx in nearest_indices]
            neighbors.append(np.mean(nearest_values))
        return np.array(neighbors)
    
    knn_matrix = find_k_nearest_neighbors(embeddings, k, x_train, x_sampled)
    # knn_matrix = np.array([np.mean(obs[indices[i]]) for i in range(len(x_train))])
    
    # 6. ä¿å­˜æ–‡ä»¶
    knn_file = f"{dataset_name}_ours_icdmw_knn.pkl"
    with open(knn_file, "wb") as f:
        pickle.dump(knn_matrix, f)
        f.close()

    print(f"âœ… Generated and saved KNN matrix to {knn_file}")
    return knn_matrix



# ==================== ä¸»ç¨‹åº ====================
if __name__ == "__main__":
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_args()
    dataset_name = args.dataset
    n_trials = args.trials
    batch_size = args.batch_size
    batch_size_prop = args.batch_size_prop
    sampling_rate = args.sampling_rate
    if torch.cuda.is_available():
        torch.cuda.set_device(args.gpu)

    print(f"\n{'='*60}")
    print(f"ğŸš€ UMVUE-DR Hyperparameter Optimization")
    print(f"ğŸ“Š Dataset: {dataset_name}")
    print(f"ğŸ”„ Trials: {n_trials}")
    print(f"ğŸ“ˆ Sampling Rate: {sampling_rate}")
    print(f"{'='*60}\n")

# ==================== æ•°æ®åŠ è½½ ====================

if dataset_name == "coat":
    train_mat, test_mat = load_data("coat")        
    x_train, y_train = rating_mat_to_sample(train_mat)
    x_test, y_test = rating_mat_to_sample(test_mat)
    num_user = train_mat.shape[0]
    num_item = train_mat.shape[1]

    y_train = binarize(y_train)
    y_test = binarize(y_test)

elif dataset_name == "yahoo":
    x_train, y_train, x_test, y_test = load_data("yahoo")
    x_train, y_train = shuffle(x_train, y_train)
    x_train[:, 0] -= 1
    x_train[:, 1] -= 1
    x_test[:, 0] -= 1
    x_test[:, 1] -= 1
    num_user = x_train[:,0].max() + 1
    num_item = x_train[:,1].max() + 1

    y_train = binarize(y_train)
    y_test = binarize(y_test)

elif dataset_name == "kuai":
    rdf_train = np.array(pd.read_table("./data/kuai/user.txt", header = None, sep = ','))     
    rdf_test = np.array(pd.read_table("./data/kuai/random.txt", header = None, sep = ','))
    rdf_train_new = np.c_[rdf_train, np.ones(rdf_train.shape[0])]
    rdf_test_new = np.c_[rdf_test, np.zeros(rdf_test.shape[0])]
    rdf = np.r_[rdf_train_new, rdf_test_new]
    
    rdf = rdf[np.argsort(rdf[:, 0])]
    c = rdf.copy()
    for i in range(rdf.shape[0]):
        if i == 0:
            c[:, 0][i] = i
            temp = rdf[:, 0][0]
        else:
            if c[:, 0][i] == temp:
                c[:, 0][i] = c[:, 0][i-1]
            else:
                c[:, 0][i] = c[:, 0][i-1] + 1
            temp = rdf[:, 0][i]
    
    c = c[np.argsort(c[:, 1])]
    d = c.copy()
    for i in range(rdf.shape[0]):
        if i == 0:
            d[:, 1][i] = i
            temp = c[:, 1][0]
        else:
            if d[:, 1][i] == temp:
                d[:, 1][i] = d[:, 1][i-1]
            else:
                d[:, 1][i] = d[:, 1][i-1] + 1
            temp = c[:, 1][i]

    y_train = d[:, 2][d[:, 3] == 1]
    y_test = d[:, 2][d[:, 3] == 0]
    x_train = d[:, :2][d[:, 3] == 1]
    x_test = d[:, :2][d[:, 3] == 0]
    
    num_user = x_train[:,0].max() + 1
    num_item = x_train[:,1].max() + 1

    y_train = binarize(y_train, 2)
    y_test = binarize(y_test, 2)
    num_user = int(num_user)
    num_item = int(num_item)

    print(f"[train] num data: {len(x_train)}")
    print(f"[test]  num data: {len(x_test)}")

else:
    print("Cant find the data set",dataset_name)

# ==================== æ•°æ®é‡‡æ · ====================
if sampling_rate < 1.0:
    print(f"_sampling training data with rate: {sampling_rate}")
    num_samples = int(len(x_train) * sampling_rate)
    indices = np.random.choice(len(x_train), num_samples, replace=False)
    x_train = x_train[indices]
    y_train = y_train[indices]
    print(f"[sampled train] num data: {len(x_train)}")




# éœ€è¦é¢„è®­ç»ƒçš„embedding
u_emb, i_emb, u_emb_test, i_emb_test = generate_embeddings(
    x_train, y_train, x_test, y_test,
    num_user, num_item,
    embedding_k=32,
    dataset_name=dataset_name
)

knn_matrix = generate_knn_matrix(
    dataset_name, x_train, y_train, x_test, y_test,
    num_user, num_item, u_emb, i_emb, k=10
)

# è¾“å‡ºæ–‡ä»¶
sampling_suffix = "" if sampling_rate == 1.0 else f"_sampling{int(sampling_rate*100)}"
output_file = f'results/{dataset_name}_UMVUE_DR_results{sampling_suffix}.txt'

# ==================== Objective å‡½æ•° ====================

def objective(trial):
    # ========== è¶…å‚æ•°é‡‡æ · ==========
    params = {
        # weight decay 
        'pred_lamb': trial.suggest_float('pred_lamb', 1e-6, 5e-3),
        'impu_lamb': trial.suggest_float('impu_lamb', 1e-6, 5e-3),
        'prop_lamb': trial.suggest_float('prop_lamb', 1e-6, 5e-3),
        
        # å­¦ä¹ ç‡
        'pred_lr': trial.suggest_categorical('pred_lr', [0.005, 0.01, 0.02, 0.05, 0.1]),
        'impu_lr': trial.suggest_categorical('impu_lr', [0.005, 0.01, 0.02, 0.05, 0.1]),
        'prop_lr': trial.suggest_categorical('prop_lr', [0.005, 0.01, 0.02, 0.05, 0.1]),

        # æ¨¡å‹ç»“æ„
        'G': trial.suggest_int('G', 2, 5), # å›¾ä¼ æ’­å±‚æ•°
        'emb': trial.suggest_categorical('emb', [4, 8, 16]),
        
        # UMVUE-DR ç‰¹æœ‰å‚æ•°
        'alpha': trial.suggest_float('alpha', 0.1, 10), # å€¾å‘æ€§è¯„åˆ†æƒé‡
        'beta': trial.suggest_float('beta', 0.0, 1.0), # è‡ªç›¸å…³åå·®æƒé‡
        'theta': trial.suggest_float('theta', 0.0, 1.0), # ä¼°è®¡åå·®æƒé‡
        'gamma': trial.suggest_float('gamma', 0.01, 0.1), # å™ªå£°æŠ‘åˆ¶ç³»æ•°
        'k': trial.suggest_categorical('k', [5, 10, 15, 20, 25]), # KNNé‚»å±…æ•°
    }
    
    # ========== æ¨¡å‹è®­ç»ƒ ==========
    model = UMVUE_DR(
        num_user, num_item,
        batch_size=batch_size,
        batch_size_prop=batch_size_prop,
        embedding_k=params['emb']
    )
    model.cuda()
    
    start = time.time()
    epoch = model.fit(
        x_train, y_train, knn_matrix, params['k'],
        G=params['G'],
        alpha=params['alpha'],
        beta=params['beta'],
        gamma=params['gamma'],
        pred_lr=params['pred_lr'],
        impu_lr=params['impu_lr'],
        prop_lr=params['prop_lr'],
        pred_lamb=params['pred_lamb'],
        impu_lamb=params['impu_lamb'],
        prop_lamb=params['prop_lamb'],
        tol=1e-5,
        verbose=False
    )
    training_time = time.time() - start
    
    # æ‰“å°è®­ç»ƒä¿¡æ¯
    print(f"Trial {trial.number}: {training_time:.2f}s, converged at epoch {epoch}")
    
    # ========== æ¨¡å‹è¯„ä¼° ==========
    test_pred = model.predict(x_test)
    
    # ç¡®ä¿æ˜¯ numpy array
    if isinstance(test_pred, torch.Tensor):
        test_pred = test_pred.cpu().numpy()
    
    # ========== æ ¹æ®æ•°æ®é›†è®¾ç½® top_k_list ==========
    if dataset_name in ['coat', 'yahoo']:
        top_k_list = [1, 3, 5, 10]
    else:  # kuai
        top_k_list = [10, 30, 50, 100]
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    mse = mse_func(y_test, test_pred)
    auc = roc_auc_score(y_test, test_pred)
    ndcg_res = ndcg_func(model, x_test, y_test, top_k_list=top_k_list)
    recall_res = recall_func(model, x_test, y_test, top_k_list=top_k_list)
    precision_res = precision_func(model, x_test, y_test, top_k_list=top_k_list)
    f1_res = f1_func(model, x_test, y_test, top_k_list=top_k_list)
    
    # ========== æ ¹æ®æ•°æ®é›†è®¾ç½®ç›®æ ‡å€¼ ==========
    if dataset_name in ['coat', 'yahoo']:
        objective_value = auc + np.mean(ndcg_res["ndcg_5"])
        # æ•´ç†ç»“æœ
        results = {
            'mse': mse,
            'auc': auc,
            'ndcg@1': np.mean(ndcg_res["ndcg_1"]),
            'ndcg@3': np.mean(ndcg_res["ndcg_3"]),
            'ndcg@5': np.mean(ndcg_res["ndcg_5"]),
            'ndcg@10': np.mean(ndcg_res["ndcg_10"]),
            'recall@1': np.mean(recall_res["recall_1"]),
            'recall@3': np.mean(recall_res["recall_3"]),
            'recall@5': np.mean(recall_res["recall_5"]),
            'recall@10': np.mean(recall_res["recall_10"]),
            'precision@1': np.mean(precision_res["precision_1"]),
            'precision@3': np.mean(precision_res["precision_3"]),
            'precision@5': np.mean(precision_res["precision_5"]),
            'precision@10': np.mean(precision_res["precision_10"]),
            'f1@1': np.mean(f1_res["f1_1"]),
            'f1@3': np.mean(f1_res["f1_3"]),
            'f1@5': np.mean(f1_res["f1_5"]),
            'f1@10': np.mean(f1_res["f1_10"]),
            'objective': objective_value
        }
    else:  # kuai
        objective_value = auc + np.mean(ndcg_res["ndcg_50"])
        # æ•´ç†ç»“æœ
        results = {
            'mse': mse,
            'auc': auc,
            'ndcg@10': np.mean(ndcg_res["ndcg_10"]),
            'ndcg@30': np.mean(ndcg_res["ndcg_30"]),
            'ndcg@50': np.mean(ndcg_res["ndcg_50"]),
            'ndcg@100': np.mean(ndcg_res["ndcg_100"]),
            'recall@10': np.mean(recall_res["recall_10"]),
            'recall@30': np.mean(recall_res["recall_30"]),
            'recall@50': np.mean(recall_res["recall_50"]),
            'recall@100': np.mean(recall_res["recall_100"]),
            'precision@10': np.mean(precision_res["precision_10"]),
            'precision@30': np.mean(precision_res["precision_30"]),
            'precision@50': np.mean(precision_res["precision_50"]),
            'precision@100': np.mean(precision_res["precision_100"]),
            'f1@10': np.mean(f1_res["f1_10"]),
            'f1@30': np.mean(f1_res["f1_30"]),
            'f1@50': np.mean(f1_res["f1_50"]),
            'f1@100': np.mean(f1_res["f1_100"]),
            'objective': objective_value
        }

    # ========== ä¿å­˜ç»“æœ ==========
    save_results(output_file, trial.number, params, results, training_time, epoch)
    
    return objective_value


# ==================== è¿è¡Œä¼˜åŒ– ====================

print(f"\n{'='*80}")
print(f"ğŸš€ Starting UMVUE-DR Hyperparameter Optimization on {dataset_name}")
print(f"ğŸ“ˆ Sampling Rate: {sampling_rate}")
print(f"{'='*80}\n")

# ç¦ç”¨ Optuna é»˜è®¤æ—¥å¿—
optuna.logging.set_verbosity(optuna.logging.WARNING)

# åˆ›å»º study
study = optuna.create_study(direction='maximize')

# è¿è¡Œä¼˜åŒ–
study.optimize(objective, n_trials=n_trials)

# ==================== è¾“å‡ºæœ€ç»ˆç»“æœ ====================

print('\n' + '='*80)
print('ğŸ‰ Optimization Completed!')
print('='*80)
print(f'Total trials: {len(study.trials)}')
print(f'Best value: {study.best_trial.value:.6f}')
print(f'\nBest parameters (Trial #{study.best_trial.number}):')
print('-'*80)
for key, value in study.best_trial.params.items():
    if isinstance(value, float):
        print(f'  {key:12s}: {value:.6f}')
    else:
        print(f'  {key:12s}: {value}')
print('='*80)
print(f"\nâœ… Results saved to: {output_file}")

# ä¿å­˜æœ€ä½³ç»“æœæ‘˜è¦
save_best_summary(output_file, study)


# python run.py --dataset coat --batch_size 128 --trials 100  --gpu 1 --sampling_rate 0.8